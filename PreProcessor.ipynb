{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: click in d:\\anaconda\\envs\\tensorflow\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\envs\\tensorflow\\lib\\site-packages (from nltk) (1.0.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.55.0-py2.py3-none-any.whl (68 kB)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=58f8366e3a7fa9024f60bf3bd769ce07acd1997c2a65c2c372b936eaaff2ed66\n",
      "  Stored in directory: c:\\users\\aman aryan\\appdata\\local\\pip\\cache\\wheels\\45\\6c\\46\\a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.5 regex-2020.11.13 tqdm-4.55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aman\n",
      "[nltk_data]     Aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aman Aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aman\n",
      "[nltk_data]     Aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49714 sha256=71c8e181c91896654546aec1c5c2dea86f596cab222acffd410472a1d3df1121\n",
      "  Stored in directory: c:\\users\\aman aryan\\appdata\\local\\pip\\cache\\wheels\\4e\\bf\\6b\\2e22b3708d14bf6384f862db539b044d6931bd6b14ad3c9adc\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.6.0\n",
      "Collecting tweet-preprocessor\n",
      "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: tweet-preprocessor\n",
      "Successfully installed tweet-preprocessor-0.6.0\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.5-py2.py3-none-any.whl (242 kB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\n",
      "Collecting jdcal\n",
      "  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Building wheels for collected packages: et-xmlfile\n",
      "  Building wheel for et-xmlfile (setup.py): started\n",
      "  Building wheel for et-xmlfile (setup.py): finished with status 'done'\n",
      "  Created wheel for et-xmlfile: filename=et_xmlfile-1.0.1-py3-none-any.whl size=8913 sha256=da7e809717d4299c809c12831070102b1c3a321275e89eed9cb175cb0e8696f2\n",
      "  Stored in directory: c:\\users\\aman aryan\\appdata\\local\\pip\\cache\\wheels\\e2\\bd\\55\\048b4fd505716c4c298f42ee02dffd9496bb6d212b266c7f31\n",
      "Successfully built et-xmlfile\n",
      "Installing collected packages: jdcal, et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.0.1 jdcal-1.4.1 openpyxl-3.0.5\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "from nltk.corpus            import wordnet\n",
    "from nltk.corpus            import stopwords\n",
    "from nltk.stem.wordnet      import WordNetLemmatizer\n",
    "import json\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import emoji\n",
    "except:\n",
    "    !pip install emoji\n",
    "    \n",
    "    import emoji\n",
    "    \n",
    "!pip install tweet-preprocessor\n",
    "    \n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_read_file(file_location):\n",
    "    if(file_location.endswith('csv')):\n",
    "        return pd.read_csv( file_location , engine = 'python')\n",
    "    elif (file_location.endswith('tsv')):\n",
    "        return pd.read_csv( file_location , engine = 'python' ,sep = '\\t')\n",
    "    elif (file_location.endswith('json')):\n",
    "        return json.load( open(file_location,'r',encoding = 'utf8'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocess(string, stop_words):\n",
    "    string        = string.lower()\n",
    "    string        = string.replace(\"â€™\", \"'\")\n",
    "    string        = string.replace(\"n\\'t\", \" not\")\n",
    "    string        = string.replace(\"'re\" , \"\")\n",
    "    string        = string.replace(\"'ve\" , \"\")\n",
    "    string        = string.replace(\"'s\"  , \"\")\n",
    "    string        = string.replace(\"'m\"  , \"\")\n",
    "    string        = string.replace(\"'ll\" , \"\")\n",
    "    string        = string.replace(\"'ve\" , \"\")\n",
    "    string        = re.sub(r'[^\\w\\s\\\\d]' , \" \" , string)\n",
    "    string        = re.sub(r'[\\d]' , ' ' , string)\n",
    "    string        = string.split(' ')\n",
    "    tokens        = [token for token in string if (len(token) > 2) and (token not in stop_words) ]\n",
    "    return tokens\n",
    "\n",
    "def string_list_preprocess(string_list, stopwords):\n",
    "    return [ string_preprocess(string, stop_words) for string in string_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag == 'J':\n",
    "        return 'a'\n",
    "    elif tag == 'R':\n",
    "        return 'r'\n",
    "    elif tag == 'V':\n",
    "        return 'v'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def string_lemmatize(tokens , lem):\n",
    "    temp_list = ''\n",
    "    tagged_words = nltk.pos_tag(tokens)\n",
    "    for word in tagged_words:\n",
    "        x =  lem.lemmatize( word[0], get_wordnet_pos( word[1][0] ) )\n",
    "        temp_list = temp_list + ' ' + x\n",
    "    return temp_list\n",
    "\n",
    "def string_list_lemmatize(tokens_list):\n",
    "    lemm_list = []\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    for tokens in tokens_list:\n",
    "        temp_list = string_lemmatize(tokens, lem)\n",
    "        lemm_list.append(temp_list)\n",
    "    return lemm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_boolean(df):\n",
    "    map_value = { 'n' : 0 , 'y' : 1 }\n",
    "    for trait in traits:\n",
    "        df[trait] = df[trait].map(map_value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_driver(file_location, stop_words):\n",
    "    in_data                = nl_read_file(file_location)\n",
    "    in_strings             = in_data['TEXT']\n",
    "    out_strings            = string_list_preprocess(in_strings , stop_words)\n",
    "    out_lemmatized_strings = string_list_lemmatize(out_strings)\n",
    "    in_data['TEXT']        = out_lemmatized_strings\n",
    "    return in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = json.load( open('input_files/emoji_dict.json','r',encoding = 'utf8'))  \n",
    "def replace_emoji_text(text):\n",
    "    for x in emoji_dict.keys():\n",
    "        text = text.replace(x,emoji_dict[x])\n",
    "    return text\n",
    "\n",
    "\n",
    "nrc_lex = pd.read_excel (r'input_files/nrc_emotion_lex.xlsx', engine='openpyxl')\n",
    "nrc_en_lex = list(nrc_lex['English (en)'])\n",
    "\n",
    "def match_text( tweet_txt ):\n",
    "    tweet_list = tweet_txt.split()\n",
    "    intersects = list(set(tweet_list) & set(nrc_en_lex))\n",
    "    if not intersects:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets( tweets ):\n",
    "    \n",
    "    import preprocessor as p\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.NUMBER)\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    tweets_cleaned = {}\n",
    "\n",
    "    for person in tweets.keys():\n",
    "\n",
    "        text_dict = ''\n",
    "        for dates in tweets[person].keys():\n",
    "            \n",
    "            string = emoji.demojize(  tweets[person][dates] )            \n",
    "            string = replace_emoji_text(string)\n",
    "            string = p.clean( string )\n",
    "            string = string.replace('#','')\n",
    "            string = string.replace('_',' ')\n",
    "            string = string_preprocess( string , stop_words)\n",
    "            string = string_lemmatize(string , lem)\n",
    "\n",
    "            if( match_text(string) and len(string.split()) > 5):\n",
    "                text_dict = text_dict + '.' + string\n",
    "\n",
    "        tweets_cleaned[person] = text_dict[2:]\n",
    "        \n",
    "    return tweets_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESSAY DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))  #- set(['not','some'])\n",
    "traits     = ['cEXT' , 'cNEU' , 'cAGR' , 'cCON' , 'cOPN' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df      = preprocess_driver( 'input_files/essays.csv', stop_words)\n",
    "out_df  = convert_to_boolean( df )\n",
    "out_df.to_csv( 'processed_datasets/essays.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tweets_pre        = 'extract/tweets_pre_covid.json'\n",
    "in_tweets_post       = 'extract/tweets_post_covid.json'\n",
    "output_tweet_loc     = 'processed_datasets/tweets_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pre = nl_read_file(in_tweets_pre)\n",
    "pre_tweets = preprocess_tweets( tweets_pre )\n",
    "\n",
    "tweets_post = nl_read_file(in_tweets_post)\n",
    "post_tweets = preprocess_tweets( tweets_post )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "pre_list = []\n",
    "post_list = []\n",
    "for person in pre_tweets.keys():\n",
    "    ids.append(person)\n",
    "    pre_list.append( pre_tweets[person] )\n",
    "    post_list.append( post_tweets[person] )\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['PERSON'] = ids\n",
    "df['PRETEXT'] = pre_list\n",
    "df['POSTTEXT'] = post_list\n",
    "\n",
    "df.to_csv(output_tweet_loc , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
